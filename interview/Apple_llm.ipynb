{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LLM(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def next_token_logits(self, prev_token_ids: list[int]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Compute probability distribution for the next token conditioned on previous tokens.\n",
    "\n",
    "        This function makes an http request to a server that can serve many requests in parallel.\n",
    "\n",
    "        Args:\n",
    "            prev_token_ids (list[int]): Tokens generated so far.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Logits for the next token. Length equal to the vocabulary size.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_stop_token_id(self) -> int:\n",
    "        \"\"\"\n",
    "        Retrieve the token that signals the end of generation.\n",
    "\n",
    "        Returns:\n",
    "            int: The integer token ID that indicates termination.\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff1a16",
   "metadata": {},
   "source": [
    " PART 1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_ids(input_token_ids: list[int], llm: LLM) -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate tokens autoregressively using a language model until the stop token is\n",
    "    encountered.\n",
    "\n",
    "    Args:\n",
    "        input_token_ids (list[int]): Token IDs for the prompt.\n",
    "        llm (LLM): The language model.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of generated token IDs, including the stop token at the end.\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x: list[float]) -> list[float]:\n",
    "    x = np.array(x)\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    return list(e / np.sum(e))\n",
    "\n",
    "def generate_token_ids(input_token_ids: list[int], llm: LLM) -> list[int]:\n",
    "    id_stop_token = llm.get_stop_token_id()\n",
    "    \n",
    "    while input_token_ids[-1] != id_stop_token:\n",
    "\n",
    "        logits = llm.next_token_logits(input_token_ids)\n",
    "        prob = softmax(logits)\n",
    "        # sample some int from prob\n",
    "        id_next_token = np.random.choice(np.arange(len(logits)), p=prob)\n",
    "        input_token_ids.append(id_next_token)\n",
    "\n",
    "    return input_token_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed226b",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_ids_multiple(input_token_ids: list[int], llm: LLM, n: int) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Generate multiple i.i.d. sequences from a language model.\n",
    "\n",
    "    Args:\n",
    "        input_token_ids (list[int]): Token IDs for the prompt.\n",
    "        llm (LLM): The language model.\n",
    "        n (int): Number of i.i.d. sequences to return.\n",
    "\n",
    "    Returns:\n",
    "        list[list[int]]: A list of lists of generated token IDs. Length n.\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc083525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def generate_token_ids_multiple(input_token_ids: list[int], llm: LLM, n: int) -> list[list[int]]:\n",
    "    out = []\n",
    "    for rollout in range(n):\n",
    "        np.random.seed(rollout)\n",
    "        start_tokens = copy(input_token_ids)\n",
    "        out.append(generate_token_ids(start_tokens, llm))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31205e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def generate_token_ids_multiple(input_token_ids: list[int], llm: LLM, n: int) -> list[list[int]]:\n",
    "    # Optimized version: llm.next_token_logits can be run in parallel\n",
    "    id_stop_token = llm.get_stop_token_id()\n",
    "    \n",
    "    tokens = []\n",
    "    for _ in range(n):\n",
    "        tokens.append( copy(input_token_ids) )\n",
    "    \n",
    "    not_done = list(range(n)) # id of the sequences that have not reached the stop token yet\n",
    "    \n",
    "    while len(not_done) > 0:\n",
    "        # generate next logits in parallel for all sequences\n",
    "        logits = await asyncio.gather([llm.next_token_logits(tokens[i]) for i in not_done])\n",
    "        probs = [softmax(logit) for logit in logits]\n",
    "\n",
    "        for i in range(len(not_done)):\n",
    "            np.random.seed(i)\n",
    "            id_next_token = np.random.choice(np.arange(len(logits[not_done[i]])), p=probs[i])\n",
    "            tokens[not_done[i]].append(id_next_token)\n",
    "            if id_next_token == id_stop_token:\n",
    "                not_done.remove(i) # stop generation of sequence i when reaching the stop token\n",
    "    \n",
    "    return tokens\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

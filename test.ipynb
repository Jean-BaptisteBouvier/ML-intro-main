{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fb57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73059080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model:int, num_heads:int, dropout: float = 0.1, bias:bool = True):\n",
    "        \"\"\"\n",
    "        Args: - d_model: size of the model (must be divisible by num_heads)\n",
    "                - num_heads: number of heads\n",
    "                - dropout frequency\n",
    "                - whether to have a bias in the projection layers\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model//num_heads\n",
    "        self.scaling = math.sqrt(self.d_head)\n",
    "\n",
    "        # Projection layers\n",
    "        self.Q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.K_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.V_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Extra layers\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, sequence:torch.Tensor, key_values:torch.Tensor = None, attn_mask: torch.Tensor = None):\n",
    "        \"\"\" self-attention or cross-attention\n",
    "        sequence: batch_size, seq_length, d_model\n",
    "        \"\"\"\n",
    "        assert sequence.shape[2] == self.d_model\n",
    "        assert len(sequence.shape) == 3\n",
    "        cross_attention = key_values is not None\n",
    "        if cross_attention:\n",
    "            assert len(key_values.shape) == 3 and key_values.shape[2] == self.d_model\n",
    "\n",
    "        # projections\n",
    "        Q = self.Q_proj(sequence) # b, l, d_model\n",
    "        if cross_attention:\n",
    "            K = self.K_proj(key_values)\n",
    "            V = self.V_proj(key_values)\n",
    "        else:\n",
    "            K = self.K_proj(sequence)\n",
    "            V = self.V_proj(sequence)\n",
    "\n",
    "        # print(K.shape)\n",
    "        # Split heads\n",
    "        Q = rearrange(Q, \"b l (h d) -> b l h d\", h=self.num_heads) # d = self.d_head\n",
    "        K = rearrange(K, \"b l (h d) -> b l h d\", h=self.num_heads) # d = self.d_head\n",
    "        V = rearrange(V, \"b l (h d) -> b l h d\", h=self.num_heads) # d = self.d_head\n",
    "        # print(K.shape)\n",
    "        attn_mat = Q @ K.transpose(-1, -2) / self.scaling # b l h h\n",
    "        # print(attn_mat.shape)\n",
    "        if attn_mask is not None:\n",
    "            attn_mat += attn_mask\n",
    "\n",
    "        attn_score = self.dropout( self.softmax(attn_mat) )\n",
    "        attn = attn_score @ V # b l h d\n",
    "        # print(attn.shape)\n",
    "        # Merge the heads\n",
    "        attn = rearrange(attn, \"b l h d -> b l (h d)\")\n",
    "        # Final projection\n",
    "        attn = self.out_proj(attn)\n",
    "        return attn\n",
    "\n",
    "\n",
    "d_model = 64\n",
    "attn = Attention(d_model=d_model, num_heads=2)\n",
    "batch_size = 4\n",
    "seq_length = 100\n",
    "x = torch.randn((batch_size, seq_length, d_model))\n",
    "\n",
    "attn = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff8394d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ae60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int):\n",
    "        super(FFN, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.net = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(),\n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" x: batch, seq_len, d_model\"\"\"\n",
    "        assert len(x.shape) == 3 and x.shape[2] == self.d_model\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436e2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" Self-Attention, Dropout, Residual, LayerNorm,\n",
    "        FFN, Dropout, Residual, LayerNorm \"\"\"\n",
    "    def __init__(self, d_model:int, num_heads:int, d_ff:int, dropout:float = 0.1, bias:bool = True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn = Attention(d_model, num_heads, dropout, bias)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.ff_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, encoded_inputs:torch.Tensor, padding_mask:torch.Tensor = None):\n",
    "        \"\"\" encoded_inputs: batch, seq_lenght, d_model \"\"\"\n",
    "        x = self.attn(encoded_inputs, padding_mask)\n",
    "        x = self.dropout(x) + encoded_inputs\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = self.dropout( self.ffn(x) ) + x\n",
    "        return self.ff_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cd1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0000e+09],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Self-Attention with causal mask, Dropout, Residual, LayerNorm\n",
    "       Cross-Attention with padding mask, Dropout, Residual, LayerNorm\n",
    "       FFN, Dropout, Residual, LayerNorm\"\"\"\n",
    "    def __init__(self, d_model:int, num_heads:int, d_ff:int, dropout:float = 0.1, bias:bool = True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attention = Attention(d_model, num_heads, dropout, bias)\n",
    "        self.self_norm = nn.LayerNorm(d_model)\n",
    "        self.cross_attention = Attention(d_model, num_heads, dropout, bias)\n",
    "        self.cross_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.ff_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, decoder_input:torch.Tensor, encoder_output:torch.Tensor, padding_mask:torch.Tensor = None):\n",
    "        \"\"\" decoder_input: b, seq_len, d_model\n",
    "            encoder_output: b, enc_seq_len, d_model\n",
    "        \"\"\"\n",
    "        seq_len = decoder_input.shape[1]\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)*-10**9\n",
    "\n",
    "        x = self.self_attention(decoder_input, key_values=None, attn_mask=causal_mask)\n",
    "        x = self.dropout(x) + decoder_input\n",
    "        x = self.self_norm(x)\n",
    "\n",
    "        y = self.cross_attention(x, key_values=encoder_output, attn_mask=padding_mask)\n",
    "        y = self.dropout(y) + x\n",
    "        y = self.cross_norm(y)\n",
    "\n",
    "        z = self.ffn(y)\n",
    "        z = self.dropout(z) + y\n",
    "        z = self.ff_norm(z)\n",
    "        return z\n",
    "    \n",
    "T = torch.triu(torch.ones(5, 5), diagonal=1)*-10**9\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_layers:int, d_model:int, num_heads:int, d_ff:int, dropout:float = 0.1, bias:bool = True):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList([Encoder(d_model, d_ff, num_heads, dropout, bias) for _ in range(num_layers)])\n",
    "        self.decoders = nn.ModuleList([Decoder(d_model, d_ff, num_heads, dropout, bias) for _ in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, embedded_encoder_input:torch.Tensor, embedded_decoder_input:torch.Tensor, padding_mask:torch.Tensor = None):\n",
    "        \"\"\" b, seq_len, d_model \"\"\"\n",
    "        encoder_output = embedded_encoder_input # initialization\n",
    "        decoder_output = embedded_decoder_input # initialization\n",
    "        \n",
    "        for encoder, decoder in zip(self.encoders, self.decoders):\n",
    "            encoder_output = encoder(encoder_output, padding_mask)\n",
    "            decoder_output = decoder(decoder_output, encoder_output, padding_mask)\n",
    "        \n",
    "        return decoder_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

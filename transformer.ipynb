{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer implementation, more or less following\n",
    "https://huggingface.co/datasets/bird-of-paradise/transformer-from-scratch-tutorial/blob/main/Transformer_Implementation_Tutorial.ipynb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b493ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56715e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Scaled Dot Product Attention\n",
    "    Args:\n",
    "        - d_model: dimensions\n",
    "        - num_heads\n",
    "        - dropout\n",
    "        - bias\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, num_heads:int = 1, dropout:float = 0.1, bias:bool = True):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model//num_heads\n",
    "\n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scaling = 1./torch.sqrt(d_head) # scaling per-head\n",
    "\n",
    "\n",
    "    def self_attention(self, sequence:torch.Tensor, attn_mask:torch.Tensor = None):\n",
    "        \"\"\"sequence: [batch, length, d_model]\n",
    "        attn_mask forces the model to only attend to previous tokens autoregressively\"\"\"\n",
    "        b, l, d = sequence.shape()\n",
    "        assert d == self.d_model\n",
    "\n",
    "        # Projections\n",
    "        Q = self.q_proj(sequence)\n",
    "        K = self.k_proj(sequence)\n",
    "        V = self.v_proj(sequence)\n",
    "\n",
    "        # Split by head\n",
    "        Q = rearrange(Q, \"b l (h d) -> b l h d\", h=self.num_heads) # d = self.d_head\n",
    "        K = rearrange(K, \"b l (h d) -> b l h d\", h=self.num_heads)\n",
    "        V = rearrange(V, \"b l (h d) -> b l h d\", h=self.num_heads)\n",
    "\n",
    "        attn_score = Q @ K.transpose(-1,-2)/self.scaling\n",
    "        if attn_mask is not None:\n",
    "            assert len(attn_mask.shape) == 4\n",
    "            attn_score += attn_mask\n",
    "        attn_score = F.softmax(attn_score, dim=-1) \n",
    "        attn_score = self.dropout(attn_score)\n",
    "        attn_output = attn_score @ V\n",
    "\n",
    "        # Merge the heads\n",
    "        attn_output = rearrange(attn_output, \"b l h d -> b l (h d)\")\n",
    "\n",
    "        # Final projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "    def cross_attention(self, sequence:torch.Tensor, key_value:torch.Tensor, attn_mask:torch.Tensor = None):\n",
    "        \"\"\"sequence: batch_size, seq_len, d_model\n",
    "            key_value: batch_size, kv_seq_len, d_model\"\"\"\n",
    "        assert len(sequence.shape) == 3 and sequence.shape[-1] == self.d_model\n",
    "        assert len(key_value.shape) == 3 and key_value.shape[-1] == self.d_model\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_proj(sequence)\n",
    "        K = self.k_proj(key_value)\n",
    "        V = self.v_proj(key_value)\n",
    "\n",
    "        # Split the heads\n",
    "        Q = rearrange(Q, \"b l (h d) -> b l h d\", h=self.num_heads)\n",
    "        K = rearrange(K, \"b l (h d) -> b l h d\", h=self.num_heads)\n",
    "        V = rearrange(V, \"b l (h d) -> b l h d\", h=self.num_heads)\n",
    "\n",
    "        attn_score = Q @ K.transpose(-1, -2)/self.scaling\n",
    "        if attn_mask is not None:\n",
    "            attn_score += attn_mask\n",
    "        attn_score = F.softmax(attn_score, dim=-1)\n",
    "        attn_score = self.dropout(attn_score)\n",
    "        attn_output = attn_score @ V\n",
    "\n",
    "        # Merge the heads\n",
    "        attn_output = rearrange(attn_output, \"b l h d -> b l (h d)\")\n",
    "\n",
    "        # Final projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        return self.self_attention(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f97855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.net = nn.Sequential([nn.Linear(d_model, d_ff), nn.ReLU(),\n",
    "                                  nn.Linear(d_ff, d_model)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Input: batch_size, sequence length, d_model\"\"\"\n",
    "        assert len(x).shape == 3 and x.shape[-1] == self.d_model\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder consists of:\n",
    "        - Attention\n",
    "        - Residual\n",
    "        - LayerNorm\n",
    "        - Feedforward\n",
    "        - Residual\n",
    "        - LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int, num_heads:int = 1, dropout:float = 0.1, bias:bool = True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attn = TransformerAttention(d_model, num_heads, dropout, bias)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ff = FFN(d_model, d_ff)\n",
    "        self.ff_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, embedded_input):\n",
    "        \"\"\" Dimensions: [batch_size, sequence_length, d_model]  \"\"\"\n",
    "        assert len(embedded_inputs.shape) == 3 and embedded_inputs.shape[-1] == self.d_model\n",
    "        x = self.attn(embedded_inputs) # Attention\n",
    "        x = self.dropout(x) + embedded_inputs # dropout and residuals\n",
    "        x = self.attn_norm(x)\n",
    "        x = self.dropout( self.ff(x) ) + x # Feedforward, dropout and residuals\n",
    "        return self.ff_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d47cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Layers:\n",
    "    self-attention\n",
    "    residual LayerNorm\n",
    "    cross-attention\n",
    "    residual LayerNorm\n",
    "    FFN\n",
    "    residual LayerNorm\"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int, num_heads:int, dropout:float = 0.1, bias:bool = True):\n",
    "        self.d_model = d_model\n",
    "        self.self_attn = TransformerAttention(d_model, num_heads, dropout, bias)\n",
    "        self.self_norm = nn.LayerNorm(d_model)\n",
    "        self.cross_attn = TransformerAttention(d_model, num_heads, dropout, bias)\n",
    "        self.cross_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, embed_input:torch.Tensor, cross_input:torch.Tensor, padding_mask:torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        embed_input: Decoder input sequence [batch_size, seq_len, d_model]\n",
    "        cross_input: Encoder output sequence [batch_size, encoder_seq_len, d_model]\n",
    "        padding_mask: Padding mask for cross-attention [batch_size, seq_len, encoder_seq_len]\n",
    "\n",
    "        causal_attention_mask: Causal mask for self-attention [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        assert embed_input.shape[2] == self.d_model\n",
    "        assert cross_input.shape[2] == self.d_model\n",
    "        seq_len = embed_input.shape[1]\n",
    "\n",
    "        # Causal mask to attend only to past tokens\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(embed_input.device)\n",
    "        causal_mask.masked_fill_(-math.inf) # seq_len, seq_len\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) # 1, 1, seq_len, seq_len\n",
    "\n",
    "        x = self.self_attn.self_attention(embed_input, attn_mask=causal_mask) # batch_size, seq_len, d_model\n",
    "        x = self.dropout(x) + embed_input # dropout and residuals\n",
    "        x = self.self_norm(x)\n",
    "\n",
    "        y = self.cross_attn.cross_attention(x, key_value=cross_input, att_mask=padding_mask)\n",
    "        y = self.dropout(y) + x # dropout and residuals\n",
    "        y = self.cross_norm(y)\n",
    "\n",
    "        z = self.dropout( self.ffn(y) ) + y # dropout, ffn, and residuals\n",
    "        z = self.ffn_norm(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    \"\"\"Stacking Encoders and Decoders\"\"\"\n",
    "    def __init__(self, num_layers:int, d_model:int, d_ff:int,\n",
    "                 num_heads:int = 1, dropout:float = 0.1, bias:bool = True, device:str = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoders = nn.ModuleList([Encoder(d_model, d_ff, num_heads, dropout, bias) for _ in range(num_layers)]).to(device)\n",
    "        self.decoders = nn.ModuleList([Decoder(d_model, d_ff, num_heads, dropout, bias) for _ in range(num_layers)]).to(device)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, padding_mask=None):\n",
    "        \"\"\" inputs: batch_size, seq_length, d_model\"\"\"\n",
    "        encoder_output = encoder_input\n",
    "        decoder_output = decoder_input\n",
    "\n",
    "        for (encoder, decoder) in zip(self.encoders, self.decoders):\n",
    "            encoder_output = encoder(encoder_output, padding_mask)\n",
    "            decoder_output = decoder(decoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        return decoder_output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
